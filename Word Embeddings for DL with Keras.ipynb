{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(200, 32, input_length=50) \n",
    "# -> Embedding(size of the vocabulary,number of the dimensions for each word vector,length of the input sentence)\n",
    "\n",
    "#The output of the word embedding is a 2D vector where words are represented in rows, \n",
    "#whereas their corresponding dimensions are presented in columns\n",
    "\n",
    "# If you wish to directly connect your word embedding layer with a densely connected layer,\n",
    "#you first have to flatten your 2D word embeddings into 1D. \n",
    "\n",
    "#Keras can be used to either learn custom words embedding or it can be used to load pretrained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for sent in corpus:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(all_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(unique_words) + 5 # adding buffer of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43, 41, 34, 34, 2], [5, 5, 33, 30, 18, 28, 14], [32, 14, 45, 14, 41, 30], [12, 37], [40, 20, 12, 8, 18, 28, 14], [1, 35, 30, 26], [1, 42, 4, 35, 17, 2], [14, 41, 35, 37, 2], [36, 31], [48, 20, 19], [45, 9], [14, 33, 38, 14], [18, 35, 9, 28, 5, 2], [5, 2, 33, 36], [18, 43, 9, 34], [5, 31, 41, 45]]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
    "print(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The embedding layer expects sentences to be of equal size\n",
    "\n",
    "\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "\n",
    "# max with respect to word count\n",
    "longest_sentence = max(corpus, key=word_count) \n",
    "\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "length_long_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43 41 34 34  2  0  0]\n",
      " [ 5  5 33 30 18 28 14]\n",
      " [32 14 45 14 41 30  0]\n",
      " [12 37  0  0  0  0  0]\n",
      " [40 20 12  8 18 28 14]\n",
      " [ 1 35 30 26  0  0  0]\n",
      " [ 1 42  4 35 17  2  0]\n",
      " [14 41 35 37  2  0  0]\n",
      " [36 31  0  0  0  0  0]\n",
      " [48 20 19  0  0  0  0]\n",
      " [45  9  0  0  0  0  0]\n",
      " [14 33 38 14  0  0  0]\n",
      " [18 35  9 28  5  2  0]\n",
      " [ 5  2 33 36  0  0  0]\n",
      " [18 43  9 34  0  0  0]\n",
      " [ 5 31 41 45  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# pad_sequences method. \n",
    "#The first parameter is the list of encoded sentences of unequal sizes \n",
    "#the second parameter is the size of the longest sentence\n",
    "#the last parameter is padding where you specify post or pre to add padding to sentences.\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 20)             1000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 141       \n",
      "=================================================================\n",
      "Total params: 1,141\n",
      "Trainable params: 1,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WorkPC\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6948 - acc: 0.4375\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6915 - acc: 0.4375\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.6881 - acc: 0.6250\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6848 - acc: 0.6250\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6815 - acc: 0.8125\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6781 - acc: 0.8125\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6748 - acc: 0.8750\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6716 - acc: 0.9375\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6683 - acc: 0.9375\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6650 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6617 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6583 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6550 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6516 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6483 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6449 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6414 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6380 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6346 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6311 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6275 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6240 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6204 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6168 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6132 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6095 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6057 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6020 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5982 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5944 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.5905 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5866 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5826 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5786 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5746 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5705 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5664 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5623 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5581 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5539 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5496 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5453 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5410 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5367 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5323 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5278 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5234 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5189 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5144 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5099 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5053 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5007 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4961 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4915 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4869 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4822 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4775 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4728 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4681 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.4634 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4587 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4539 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4492 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4444 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4397 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4349 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4302 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4254 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4207 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4160 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4112 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4065 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4018 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3971 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3924 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3877 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3831 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3784 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.3738 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.3692 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3647 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3601 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3556 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3511 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3466 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3422 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3378 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3334 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3291 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3248 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3205 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3162 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3120 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3079 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3037 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2996 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2956 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2916 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2876 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2836 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f8349d2648>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can see that the first layer has 1000 trainable parameters.\n",
    "#This is because our vocabulary size is 50 and each word will be presented as a 20 dimensional vector\n",
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we used one_hot function to convert text to vectors.\n",
    "#Another approach is to use Tokenizer function from keras.preprocessing.text library.\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 1 to store the dimensions for the words for which no pretrained word embeddings exist.\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 3, 15, 16, 1], [4, 17, 6, 9, 5, 7, 2], [18, 19, 20, 2, 3, 21], [22, 23], [24, 25, 26, 27, 5, 7, 2], [28, 8, 9, 29], [30, 31, 32, 8, 33, 1], [2, 3, 8, 34, 1], [10, 11], [35, 36, 37], [12, 38], [2, 6, 39, 40], [5, 41, 13, 7, 4, 1], [4, 1, 6, 10], [5, 42, 13, 43], [4, 11, 3, 12]]\n"
     ]
    }
   ],
   "source": [
    "#Finally, to convert sentences to their numeric counterpart, call the texts_to_sequences function and pass it the whole corpus.\n",
    "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
    "print(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  3 15 16  1  0  0]\n",
      " [ 4 17  6  9  5  7  2]\n",
      " [18 19 20  2  3 21  0]\n",
      " [22 23  0  0  0  0  0]\n",
      " [24 25 26 27  5  7  2]\n",
      " [28  8  9 29  0  0  0]\n",
      " [30 31 32  8 33  1  0]\n",
      " [ 2  3  8 34  1  0  0]\n",
      " [10 11  0  0  0  0  0]\n",
      " [35 36 37  0  0  0  0]\n",
      " [12 38  0  0  0  0  0]\n",
      " [ 2  6 39 40  0  0  0]\n",
      " [ 5 41 13  7  4  1  0]\n",
      " [ 4  1  6 10  0  0  0]\n",
      " [ 5 42 13 43  0  0  0]\n",
      " [ 4 11  3 12  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to load the GloVe word embeddings \n",
    "# and then create our embedding matrix that contains the words in our corpus and their corresponding values from GloVe embeddings.\n",
    "embeddings_dictionary = dict() # to store our word embeddings\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values,\n",
    "#in the form of an array\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create a two dimensional numpy array of 44 (size of vocabulary) rows and 100 columns. \n",
    "#The array will initially contain zeros. The array will be named as embedding_matrix\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "\n",
    "#The 100 dimensional vectors will be stored at the corresponding index of the word in the embedding_matrix\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.38251001,  0.14821   ,  0.60601002, ...,  0.058921  ,\n",
       "         0.091112  ,  0.47283   ],\n",
       "       [-0.30664   ,  0.16821   ,  0.98510998, ..., -0.38775   ,\n",
       "         0.36916   ,  0.54521   ],\n",
       "       ...,\n",
       "       [ 0.30449   , -0.19628   ,  0.20225   , ..., -0.18385001,\n",
       "        -0.12432   ,  0.27467999],\n",
       "       [-0.26703   ,  0.44911   ,  0.55478001, ..., -0.87247002,\n",
       "         0.83828002,  0.465     ],\n",
       "       [-0.57547998, -0.043236  , -0.1972    , ..., -0.10507   ,\n",
       "         0.26554999,  0.32192999]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can pass our pretrained embedding matrix as default weights to the weights parameter.\n",
    "#And since we are not training the embedding layer, the trainable attribute has been set to False.\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 7, 100)            4400      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 701       \n",
      "=================================================================\n",
      "Total params: 5,101\n",
      "Trainable params: 701\n",
      "Non-trainable params: 4,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 44 x 100= 4400 params for 44 words and 100 dimensions\n",
    "# output shape -> for 7 words each sentence and their 100 dimensions\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8756 - acc: 0.3125\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.8446 - acc: 0.3125\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.8150 - acc: 0.3125\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.7892 - acc: 0.3125\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.7620 - acc: 0.3125\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.7360 - acc: 0.3125\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.7110 - acc: 0.3750\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6871 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6640 - acc: 0.6250\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6418 - acc: 0.6875\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6203 - acc: 0.7500\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5996 - acc: 0.8125\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5796 - acc: 0.8125\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5604 - acc: 0.8125\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.5418 - acc: 0.8125\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.5239 - acc: 0.8750\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5067 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4901 - acc: 0.8750\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4741 - acc: 0.8750\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4588 - acc: 0.8750\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4440 - acc: 0.8750\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4297 - acc: 0.8750\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 68us/step - loss: 0.4161 - acc: 0.9375\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4031 - acc: 0.9375\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3905 - acc: 0.9375\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.3785 - acc: 0.9375\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3669 - acc: 0.9375\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3558 - acc: 0.9375\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3451 - acc: 0.9375\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3348 - acc: 0.9375\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3250 - acc: 0.9375\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3155 - acc: 0.9375\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 60us/step - loss: 0.3064 - acc: 0.9375\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.2977 - acc: 0.9375\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2893 - acc: 0.9375\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.2812 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2735 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2660 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2589 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2520 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2454 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2390 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.2329 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2270 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2213 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2159 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2106 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2055 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2007 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.1960 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1914 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1870 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1828 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1788 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1748 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1710 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1674 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1638 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1604 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1571 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 60us/step - loss: 0.1539 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1508 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1478 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1449 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1421 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1393 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1367 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1341 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.1317 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 60us/step - loss: 0.1293 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1269 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1247 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1225 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1203 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1182 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1162 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.1143 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1124 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1105 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1087 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1069 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.1052 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1036 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1019 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1004 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0988 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0973 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 60us/step - loss: 0.0959 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.0944 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.0930 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.0917 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0904 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.0891 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0878 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 60us/step - loss: 0.0866 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0854 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0842 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0830 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0819 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0808 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f83798b208>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the advanced deep learning models involving multiple inputs and outputs use the Functional API.\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "deep_inputs = Input(shape=(length_long_sentence,)) # pass the length of input vector\n",
    "\n",
    "embedding = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)(deep_inputs) # line A\n",
    "\n",
    "flatten = Flatten()(embedding)\n",
    "\n",
    "hidden = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "model = Model(inputs=deep_inputs, outputs=hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 7, 100)            4400      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 701       \n",
      "=================================================================\n",
      "Total params: 5,101\n",
      "Trainable params: 701\n",
      "Non-trainable params: 4,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7299 - acc: 0.6250\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.7035 - acc: 0.5625\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6748 - acc: 0.5625\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6465 - acc: 0.5625\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.6196 - acc: 0.5625\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5944 - acc: 0.6875\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5711 - acc: 0.7500\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5494 - acc: 0.7500\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.5293 - acc: 0.8125\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.5105 - acc: 0.8750\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4929 - acc: 0.8750\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4763 - acc: 0.8750\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4604 - acc: 0.8750\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4453 - acc: 0.8750\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.4307 - acc: 0.8750\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4167 - acc: 0.8750\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.4031 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3899 - acc: 0.8750\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3772 - acc: 0.8750\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3650 - acc: 0.9375\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3533 - acc: 0.9375\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3420 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3312 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3209 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3111 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3017 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2928 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2843 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2761 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2684 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2609 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2538 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2470 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2404 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2341 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2280 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2222 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2165 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2111 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2058 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2007 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1959 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1912 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1866 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1823 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1781 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.1740 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.1701 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1664 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1628 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1593 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1559 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1527 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1495 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1465 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 64us/step - loss: 0.1435 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.1407 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1379 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1353 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1327 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1302 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1277 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1254 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1231 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1209 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1187 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1166 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1146 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1126 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1107 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1088 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.1070 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1052 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1035 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1019 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1002 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 61us/step - loss: 0.0986 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0971 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0956 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0941 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0927 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0900 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0886 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0874 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0861 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0849 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0837 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0825 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0813 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0802 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0791 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0780 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0770 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0760 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0750 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0740 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0730 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0721 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0712 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)\n",
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use text data as input to the deep learning model, we need to convert text to numbers. However unlike machine learning models, passing sparse vector of huge sizes can greately affect deep learning models. Therefore, we need to convert our text to small dense vectors. Word embeddings help us convert text to dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
